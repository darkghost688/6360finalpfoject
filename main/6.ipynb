{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Importing Necessary Libraries\n",
    "\n",
    "# Import necessary libraries\n",
    "# This step imports all the necessary Python libraries that are required for data manipulation, visualization, and modeling.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 2: Loading the Dataset\n",
    "\n",
    "# Load the dataset\n",
    "# This step loads the dataset that we are going to analyze. The dataset contains information about car features and prices.\n",
    "df = pd.read_csv('./car_price_prediction_dataset.csv')\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "\n",
    "# Handling missing values by dropping them or imputing\n",
    "# Here, we'll check for any missing values and fill them with median values for simplicity.\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Convert categorical variables to numerical values if necessary\n",
    "# Assuming that the dataset has some categorical variables, they need to be encoded.\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = pd.Categorical(df[column]).codes\n",
    "\n",
    "# Step 4: Standardizing the Data\n",
    "\n",
    "# Standardizing the dataset to ensure each feature has mean = 0 and standard deviation = 1\n",
    "# This step is crucial for models that are sensitive to the scale of data.\n",
    "def standardize_data(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "df = df.apply(standardize_data)\n",
    "\n",
    "# Step 5: Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Visualizing the distribution of MSRP and other key features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['MSRP'], kde=True, color='blue')\n",
    "plt.title('Distribution of MSRP')\n",
    "plt.xlabel('MSRP')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Splitting the Dataset\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "# This step helps in validating the model's performance on unseen data.\n",
    "X = df.drop(columns=['MSRP'])\n",
    "y = df['MSRP']\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Building Linear Models\n",
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(train_data, train_labels)\n",
    "linear_predictions = linear_model.predict(test_data)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge()\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "ridge_search = GridSearchCV(ridge, ridge_params, cv=5)\n",
    "ridge_search.fit(train_data, train_labels)\n",
    "ridge_predictions = ridge_search.best_estimator_.predict(test_data)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso()\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "lasso_search = GridSearchCV(lasso, lasso_params, cv=5)\n",
    "lasso_search.fit(train_data, train_labels)\n",
    "lasso_predictions = lasso_search.best_estimator_.predict(test_data)\n",
    "\n",
    "# Step 8: Building Nonlinear Models\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(train_data, train_labels)\n",
    "rf_predictions = rf.predict(test_data)\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(train_data, train_labels)\n",
    "gbr_predictions = gbr.predict(test_data)\n",
    "\n",
    "# Step 9: Model Evaluation\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(name, predictions, true_values):\n",
    "    rmse = mean_squared_error(true_values, predictions, squared=False)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    print(f'{name} - RMSE: {rmse:.4f}, R²: {r2:.4f}')\n",
    "\n",
    "# Evaluating Linear Models\n",
    "evaluate_model('Linear Regression', linear_predictions, test_labels)\n",
    "evaluate_model('Ridge Regression', ridge_predictions, test_labels)\n",
    "evaluate_model('Lasso Regression', lasso_predictions, test_labels)\n",
    "\n",
    "# Evaluating Nonlinear Models\n",
    "evaluate_model('Random Forest', rf_predictions, test_labels)\n",
    "evaluate_model('Gradient Boosting', gbr_predictions, test_labels)\n",
    "\n",
    "# Step 10: Analyzing Feature Importance\n",
    "\n",
    "# Analyzing Feature Importance from Gradient Boosting Regressor\n",
    "feature_importance = gbr.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance from Gradient Boosting Regressor')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Summary of Findings\n",
    "\n",
    "# Gradient Boosting Regressor showed the best performance with the lowest RMSE and highest R².\n",
    "# Key Features affecting MSRP include features with high importance in the Gradient Boosting model.\n",
    "# Future work can include further hyperparameter tuning and exploration of other nonlinear models like Neural Networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
